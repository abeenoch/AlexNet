{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alexnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "INTRODUCTION AND HISTORY TO ALEX NET\n",
        "\n",
        "AlexNet is the name of a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor. AlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012.The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training. AlexNet was not the first fast GPU-implementation of a CNN to win an image recognition contest. A CNN on GPU by K. Chellapilla et al. (2006) was 4 times faster than an equivalent implementation on CPU.A deep CNN of Dan Cireșan et al. (2011) at IDSIA was already 60 times faster and achieved superhuman performance in August 2011.Between May 15, 2011 and September 10, 2012, their CNN won no fewer than four image competitions.They also significantly improved on the best performance in the literature for multiple image databases.\n",
        "\n",
        "Implementing AlexNet is more straightforward than implementing LeNet-because the architecture elements are in line with modern neural networks.\n",
        "\n",
        "The main innovation introduced by AlexNet compared to the LeNet-5 was its sheer size. AlexNet main elements are the same: a sequence of convolutional and pooling layers followed by a couple of fully-connected layers. The LeNet-5 has two sets of convolutional and pooling layers, two fully-connected layers, and an RBD classifier as an output layer. AlexNet has five convolutional layers, three pooling layers, two fully-connected layers, and a softmax classifier output layer. The training time and dataset were larger as well. All of this was possible thanks to the availability of more computational processing power (particularly Graphics Processing Units (GPUs)), and larger datasets (because of the internet). There a few additional innovations introduced with AlexNet:\n",
        "\n",
        "Rectifier Linear Units (ReLU): instead of the hyperbolic tangent (tanh) and sigmoid units. ReLUs train several times faster than tanh or sigmoid units. Normalization layers: aimed to reduce overfitting. More on this latter. Dropout layers: dropout consists of setting to zero the output of a hidden neuron with some probability, in this case, 0.5. Also aimed to help with overfitting. Data augmentation: images were artificially translated, reflected, and distorted to increase the dataset size. The more variation in training examples, the more information available for the model to learn."
      ],
      "metadata": {
        "id": "FJuLIO8ESKtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dependencies\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras import backend as K\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "m5HTqeuWgUWw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1000)\n",
        "\n",
        "#Instantiation\n",
        "AlexNet = Sequential()"
      ],
      "metadata": {
        "id": "f31ak6t1juSg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AlexNet model architecture in Keras\n",
        "Here is where we ensemble AlexNet architecture as shown (as closely as possible) in Figure 7. The comments explain each step in the model definition.\n",
        "\n",
        "Since we are usin CIFAR-10 32x32 images instead of the 224x224 ImageNet images, “padding” will be necessary in several layers so dimensions match. Normally we will use kernels with different dimensions for CIFAR-10 but I’m opting for padding to recreate AlexNet as closely as possible."
      ],
      "metadata": {
        "id": "HTvIo-HCU-wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1st Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#2nd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#3rd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#4th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#5th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#Passing it to a Fully Connected layer\n",
        "AlexNet.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#2nd Fully Connected Layer\n",
        "AlexNet.add(Dense(4096))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#3rd Fully Connected Layer\n",
        "AlexNet.add(Dense(1000))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#Output Layer\n",
        "AlexNet.add(Dense(10))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('softmax'))\n",
        "\n",
        "#Model Summary\n",
        "AlexNet.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hn9Gp-DkEur",
        "outputId": "091005f7-db8f-41f6-e188-1209ca69fe4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 8, 8, 96)          34944     \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 8, 8, 96)         384       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 8, 96)          0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 4, 4, 96)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 4, 4, 256)         614656    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 4, 4, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 2, 2, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 2, 2, 384)         885120    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 2, 2, 384)        1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 2, 2, 384)         0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 2, 2, 384)         1327488   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 2, 2, 384)        1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 2, 2, 384)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 2, 2, 256)         884992    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 2, 2, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 2, 2, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 1, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              1052672   \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 4096)             16384     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 4096)             16384     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1000)              4097000   \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 1000)             4000      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 1000)              0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                10010     \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 10)               40        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,730,506\n",
            "Trainable params: 25,709,350\n",
            "Non-trainable params: 21,156\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "GfzU8DjSkEpC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "since im ready with my model,i checked its performance in classification.I will use the CIFAR10 dataset that is a popular benchmark in image classification. The CIFAR-10 dataset is a publically available image data set provided by the Canadian Institute for Advanced Research (CIFAR). It consists of 60000 32×32 colour images in 10 classes, with 6000 images per class. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 50000 training images and 10000 test images in this dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "-WwSWqzQUXH8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0DC7dPOBftMI"
      },
      "outputs": [],
      "source": [
        "#Keras library for CIFAR dataset\n",
        "from keras.datasets import cifar10\n",
        "(x_train, y_train),(x_test, y_test)=cifar10.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lk0amlH6YJ_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train-validation-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)\n",
        "\n",
        "#Dimension of the CIFAR10 dataset\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBz6NkuFgfhN",
        "outputId": "f823c7d5-ab72-49d9-b205-0946c921e885"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((35000, 32, 32, 3), (35000, 1))\n",
            "((15000, 32, 32, 3), (15000, 1))\n",
            "((10000, 32, 32, 3), (10000, 1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding of the target vectori.e.i need to reshape the target vector"
      ],
      "metadata": {
        "id": "vR5QotTLUm4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Onehot Encoding the labels.\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10\n",
        "y_train=to_categorical(y_train)\n",
        "y_val=to_categorical(y_val)\n",
        "y_test=to_categorical(y_test)\n",
        "\n",
        "#Verifying the dimension after one hot encoding\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpkFaZIxgfcU",
        "outputId": "2d07cb75-a141-483c-b0ff-8a2cfc037f59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((35000, 32, 32, 3), (35000, 10))\n",
            "((15000, 32, 32, 3), (15000, 10))\n",
            "((10000, 32, 32, 3), (10000, 10))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Image Data Augmentation\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1 )\n",
        "\n",
        "val_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1)\n",
        "\n",
        "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True,zoom_range=.1)\n",
        "\n",
        "#Fitting the augmentation defined above to the data\n",
        "train_generator.fit(x_train)\n",
        "val_generator.fit(x_val)\n",
        "test_generator.fit(x_test)"
      ],
      "metadata": {
        "id": "YCUzmqAMhf2b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After preprocessing the CIFAR10 dataset, we are ready now to train our defined AlexNet model. We will use the learning rate annealer in this experiment. The learning rate annealer decreases the learning rate after a certain number of epochs if the error rate does not change. Here, through this technique, we will monitor the validation accuracy and if it seems to be a plateau in 3 epochs, it will reduce the learning rate by 0.01."
      ],
      "metadata": {
        "id": "17CYZICUXuA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Learning Rate Annealer\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "lrr= ReduceLROnPlateau(   monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5)"
      ],
      "metadata": {
        "id": "dw32kk8zhqQ3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model,i had to define below the number of epochs, the number of batches and the learning rate."
      ],
      "metadata": {
        "id": "v0N2j71kXnME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the parameters\n",
        "batch_size= 250\n",
        "epochs=50\n",
        "learn_rate=.001"
      ],
      "metadata": {
        "id": "im8muUOlhqLy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "AlexNet.fit_generator(train_generator.flow(x_train, y_train, batch_size=batch_size), epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, validation_data = val_generator.flow(x_val, y_val, batch_size=batch_size), validation_steps = 250, callbacks = [lrr], verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8Ynd2Dkhz7B",
        "outputId": "1d194e80-99e5-43d1-9131-2fa9c1e5a17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140/140 [==============================] - ETA: 0s - loss: 1.6293 - accuracy: 0.4204WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
            "140/140 [==============================] - 616s 4s/step - loss: 1.6293 - accuracy: 0.4204 - val_loss: 2.1407 - val_accuracy: 0.2023 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - ETA: 0s - loss: 1.3972 - accuracy: 0.5121WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "140/140 [==============================] - 568s 4s/step - loss: 1.3972 - accuracy: 0.5121 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - ETA: 0s - loss: 1.2794 - accuracy: 0.5582WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "140/140 [==============================] - 592s 4s/step - loss: 1.2794 - accuracy: 0.5582 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - ETA: 0s - loss: 1.1937 - accuracy: 0.5915WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "140/140 [==============================] - 579s 4s/step - loss: 1.1937 - accuracy: 0.5915 - lr: 0.0010\n",
            "Epoch 5/50\n",
            " 22/140 [===>..........................] - ETA: 8:06 - loss: 1.1085 - accuracy: 0.6242"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#After successful training, we will visualize its performance.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#Plotting the training and validation loss\n",
        "\n",
        "f,ax=plt.subplots(2,1) #Creates 2 subplots under 1 column\n",
        "\n",
        "#Assigning the first subplot to graph training loss and validation loss\n",
        "ax[0].plot(AlexNet.history.history['loss'],color='b',label='Training Loss')\n",
        "ax[0].plot(AlexNet.history.history['val_loss'],color='r',label='Validation Loss')\n",
        "\n",
        "#Plotting the training accuracy and validation accuracy\n",
        "ax[1].plot(AlexNet.history.history['accuracy'],color='b',label='Training  Accuracy')\n",
        "ax[1].plot(AlexNet.history.history['val_accuracy'],color='r',label='Validation Accuracy')\n",
        "\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "GlbFGP_DYtJD",
        "outputId": "222e9111-71ae-4b30-c9f0-bd28aaa8794e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-933e5a987645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Assigning the first subplot to graph training loss and validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAlexNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAlexNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AlexNet' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = AlexNet.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test loss score: {score[0]}')\n",
        "print(f'Test accuracy score:{ score[1]}')"
      ],
      "metadata": {
        "id": "QZlI0-pvaUX-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}